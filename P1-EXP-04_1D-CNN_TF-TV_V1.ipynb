{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: 1-Dimensional Convolutional Neural Network (1D-CNN) Model with TensorFlow TextVectorization Layer Embeddings\n",
    "**(Version 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import the necessary libraries and modules for this experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# %pip install tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as k\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default precision for numpy\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Enable display of multiple outputs per Jupyter Notebook cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2. Load in the golf course reviews dataset & create a new label column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"top_and_non_golf_course_reviews.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a new label column that indicates whether the review is a top100 course or not\n",
    "df['top100'] = df['label'].apply(lambda x: 1 if x == 'top100' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**3. Split the dataset into training, validation, and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape: (80, 11)\n",
      "Validation Dataset Shape: (20, 11)\n",
      "Test Dataset Shape: (20, 11)\n"
     ]
    }
   ],
   "source": [
    "train_df, remaining = train_test_split(df, test_size=0.33, stratify=df['top100'], random_state=42)\n",
    "val_df, test_df = train_test_split(remaining, test_size=0.5, stratify=remaining['top100'], random_state=42)\n",
    "\n",
    "# Check the shape of the training, validation, and test sets\n",
    "print(f\"Training Dataset Shape: {train_df.shape}\")\n",
    "print(f\"Validation Dataset Shape: {val_df.shape}\")\n",
    "print(f\"Test Dataset Shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**4. Convert the split DataFrames into TensorFlow Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(train_df))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(dict(val_df))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**5. Create `custom_stopwords` function and `text_vectorization` layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a `custom_stopwords` function to remove stopwords, strip punctuation, and lowercase the text\n",
    "def custom_stopwords(input_text):\n",
    "    \"\"\"\n",
    "    Removes stopwords, strips punctuation, and lowers the input text.\n",
    "\n",
    "    Args:\n",
    "        input_text (tf.Tensor): The input text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The processed text with stopwords removed, punctuation stripped, and lowercased.\n",
    "    \"\"\"\n",
    "    lowercase = tf.strings.lower(input_text)\n",
    "    stripped_punct = tf.strings.regex_replace(lowercase,\n",
    "                                            '[%s]' % re.escape(string.punctuation),\n",
    "                                            '')\n",
    "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*', \"\")\n",
    "\n",
    "# Download stopwords from the NLTK library\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "# Define the maxium sequence and token length for this experiment\n",
    "max_length =  3073\n",
    "max_tokens = 10000\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    standardize=custom_stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**6. Adapt the TextVectorization layer to a text_only_dataset of the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 12:21:32.569810: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Create a text_only_train_dataset which contains only the review_text column of the training dataset\n",
    "text_only_train_dataset = train_ds.map(lambda x: x['review_text'])\n",
    "\n",
    "# Adapt the TextVectorization layer to the text_only_train_dataset\n",
    "text_vectorization.adapt(text_only_train_dataset)\n",
    "\n",
    "# Create int_train_ds, int_val_ds, and int_test_ds from the train, val, and test datasets respectively using the TextVectorization layer\n",
    "int_train_ds = train_ds.map(lambda x: (text_vectorization(x['review_text']), x['top100']))\n",
    "int_val_ds = val_ds.map(lambda x: (text_vectorization(x['review_text']), x['top100']))\n",
    "int_test_ds = test_ds.map(lambda x: (text_vectorization(x['review_text']), x['top100']))\n",
    "\n",
    "# Batch and pad the datasets to have a sequence lenfth dimension\n",
    "batch_size = 32\n",
    "max_sequence_length = 3073\n",
    "\n",
    "int_train_ds = int_train_ds.map(lambda x, y: (x, y)).padded_batch(batch_size, padded_shapes=(max_sequence_length, ()))\n",
    "int_val_ds = int_val_ds.map(lambda x, y: (x, y)).padded_batch(batch_size, padded_shapes=(max_sequence_length, ()))\n",
    "int_test_ds = int_test_ds.map(lambda x, y: (x, y)).padded_batch(batch_size, padded_shapes=(max_sequence_length, ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**7. Create Two Layer 1D-CNN Model**\n",
    "\n",
    "- Two 1D-CNN layers (64 filters and 32 filters | Kernel Size = 5 each)\n",
    "- MaxPooling1D Layers after each 1D-CNN layer (pool_size = 2)\n",
    "- GlobalMaxPooling1D Layer after the second MaxPooling1D layer\n",
    "- Dense Layer with 64 units and ReLU activation function\n",
    "- RMSprop optimizer (learning rate = 0.001)\n",
    "- Vocabulary size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanjenss/anaconda3/envs/AI-DL/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer '1d-cnn_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model constants\n",
    "vocab_size = 10000\n",
    "embedding_dim = 356\n",
    "kernel_size = 5\n",
    "\n",
    "# Build the 1D-CNN model\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
    "embedding = layers.Embedding(input_dim=vocab_size,\n",
    "                             output_dim=embedding_dim,\n",
    "                             mask_zero=True, name=\"embedding\")(inputs)\n",
    "x = layers.Conv1D(filters=64, kernel_size=kernel_size, activation='relu', name='1d-cnn_1')(embedding)\n",
    "x = layers.MaxPooling1D(pool_size=2, name='maxpool_1')(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=kernel_size, activation='relu', name='1d-cnn_2')(x)\n",
    "x = layers.MaxPooling1D(pool_size=2, name='maxpool_2')(x)\n",
    "x = layers.GlobalMaxPooling1D(name='globalmaxpool')(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Two_Layer_1D-CNN_TF-TV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "**8. Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Two_Layer_1D-CNN_TF-TV\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Two_Layer_1D-CNN_TF-TV\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">356</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ 1d-cnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">113,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ 1d-cnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ globalmaxpool                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m356\u001b[0m)      │     \u001b[38;5;34m3,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ 1d-cnn_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │       \u001b[38;5;34m113,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ 1d-cnn_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │        \u001b[38;5;34m10,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ globalmaxpool                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,686,433</span> (14.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,686,433\u001b[0m (14.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,686,433</span> (14.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,686,433\u001b[0m (14.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**9. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.4896 - loss: 0.6956WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x34cea34c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 411ms/step - accuracy: 0.4922 - loss: 0.6954 - val_accuracy: 0.5500 - val_loss: 0.6926\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step - accuracy: 0.9336 - loss: 0.6661 - val_accuracy: 0.5000 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - accuracy: 0.9820 - loss: 0.6309 - val_accuracy: 0.5000 - val_loss: 0.6915\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436ms/step - accuracy: 1.0000 - loss: 0.5810 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
      "Training Time: 5.838586091995239 seconds\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing models in memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define the callbacks for the model training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"EXP_4_1D-CNN_TF_TV.keras\", save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = model.fit(int_train_ds,\n",
    "                    validation_data=int_val_ds,\n",
    "                    epochs=20,\n",
    "                    callbacks=callbacks)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the training time\n",
    "print(f\"Training Time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**10. Evaluate the model on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5000 - loss: 0.6889\n",
      "Test Loss: 0.6889338493347168, Test Accuracy: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1.2.2 Extract the training history and add all evaluation metrics into a history DataFrame\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXP</th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1D-CNN w/ TextVectorization Embeddings</td>\n",
       "      <td>0.570403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.690904</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.688934</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.838586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXP                                   Model  Training Loss  \\\n",
       "0    4  1D-CNN w/ TextVectorization Embeddings       0.570403   \n",
       "\n",
       "   Training Accuracy  Validation Loss  Validation Accuracy  Test Loss  \\\n",
       "0                1.0         0.690904                  0.5   0.688934   \n",
       "\n",
       "   Test Accuracy  Training Time  \n",
       "0            0.5       5.838586  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model\n",
    "model = tf.keras.models.load_model(\"EXP_4_1D-CNN_TF_TV.keras\")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(int_test_ds)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Get the training loss, validation loss, training accuracy, and validation accuracy from the history object\n",
    "training_loss = history.history['loss'][-1] # the -1 index gets the last epoch\n",
    "validation_loss = history.history['val_loss'][-1]\n",
    "training_accuracy = history.history['accuracy'][-1]\n",
    "validation_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "\"\"\"\n",
    "1.2.2 Extract the training history and add all evaluation metrics into a history DataFrame\n",
    "\"\"\"\n",
    "# Extract the training history into a pandas DataFrame\n",
    "history_df = pd.DataFrame({\n",
    "    'EXP': [4],\n",
    "    'Model': ['1D-CNN w/ TextVectorization Embeddings'],\n",
    "    'Training Loss': [training_loss],\n",
    "    'Training Accuracy': [training_accuracy],\n",
    "    'Validation Loss': [validation_loss],\n",
    "    'Validation Accuracy': [validation_accuracy],\n",
    "    'Test Loss': [test_loss],\n",
    "    'Test Accuracy': [test_accuracy],\n",
    "    'Training Time': [training_time]\n",
    "})\n",
    "\n",
    "# Inspect the history DataFrame\n",
    "history_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
