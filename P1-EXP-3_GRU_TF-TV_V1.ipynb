{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Gated Recurrent Unit (GRU) Model with TensorFlow TextVectorization Layer Embeddings\n",
    "**(Version 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import the necessary libraries and modules for this experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# %pip install tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as k\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default precision for numpy\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Enable display of multiple outputs per Jupyter Notebook cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2. Load in the golf course reviews dataset & create a new label column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"top_and_non_golf_course_reviews.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a new label column that indicates whether the review is a top100 course or not\n",
    "df['top100'] = df['label'].apply(lambda x: 1 if x == 'top100' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**3. Split the dataset into training, validation, and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape: (80, 11)\n",
      "Validation Dataset Shape: (20, 11)\n",
      "Test Dataset Shape: (20, 11)\n"
     ]
    }
   ],
   "source": [
    "train_df, remaining = train_test_split(df, test_size=0.33, stratify=df['top100'], random_state=42)\n",
    "val_df, test_df = train_test_split(remaining, test_size=0.5, stratify=remaining['top100'], random_state=42)\n",
    "\n",
    "# Check the shape of the training, validation, and test sets\n",
    "print(f\"Training Dataset Shape: {train_df.shape}\")\n",
    "print(f\"Validation Dataset Shape: {val_df.shape}\")\n",
    "print(f\"Test Dataset Shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**4. Convert the split DataFrames into TensorFlow Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(train_df))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(dict(val_df))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**5. Create `custom_stopwords` function and `text_vectorization` layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a `custom_stopwords` function to remove stopwords, strip punctuation, and lowercase the text\n",
    "def custom_stopwords(input_text):\n",
    "    \"\"\"\n",
    "    Removes stopwords, strips punctuation, and lowers the input text.\n",
    "\n",
    "    Args:\n",
    "        input_text (tf.Tensor): The input text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The processed text with stopwords removed, punctuation stripped, and lowercased.\n",
    "    \"\"\"\n",
    "    lowercase = tf.strings.lower(input_text)\n",
    "    stripped_punct = tf.strings.regex_replace(lowercase,\n",
    "                                            '[%s]' % re.escape(string.punctuation),\n",
    "                                            '')\n",
    "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*', \"\")\n",
    "\n",
    "# Download stopwords from the NLTK library\n",
    "nltk.download('stopwords', quiet=True)\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "# Define the maxium sequence and token length for this experiment\n",
    "max_length =  3073\n",
    "max_tokens = 10000\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    standardize=custom_stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**6. Adapt the TextVectorization layer to a text_only_dataset of the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 11:26:36.982598: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Create a text_only_train_dataset which contains only the review_text column of the training dataset\n",
    "text_only_train_dataset = train_ds.map(lambda x: x['review_text'])\n",
    "\n",
    "# Adapt the TextVectorization layer to the text_only_train_dataset\n",
    "text_vectorization.adapt(text_only_train_dataset)\n",
    "\n",
    "# Create int_train_ds, int_val_ds, and int_test_ds from the train, val, and test datasets respectively using the TextVectorization layer\n",
    "int_train_ds = train_ds.map(lambda x: (text_vectorization(x['review_text']), x['top100']))\n",
    "int_val_ds = val_ds.map(lambda x: (text_vectorization(x['review_text']), x['top100']))\n",
    "int_test_ds = test_ds.map(lambda x: (text_vectorization(x['review_text']), x['top100']))\n",
    "\n",
    "# Batch and pad the datasets to have a sequence lenfth dimension\n",
    "batch_size = 32\n",
    "max_sequence_length = 3073\n",
    "\n",
    "int_train_ds = int_train_ds.map(lambda x, y: (x, y)).padded_batch(batch_size, padded_shapes=(max_sequence_length, ()))\n",
    "int_val_ds = int_val_ds.map(lambda x, y: (x, y)).padded_batch(batch_size, padded_shapes=(max_sequence_length, ()))\n",
    "int_test_ds = int_test_ds.map(lambda x, y: (x, y)).padded_batch(batch_size, padded_shapes=(max_sequence_length, ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**7. Create Two Layer GRU Model**\n",
    "\n",
    "- Two GRU layers (64 units and 32 units)\n",
    "- ReLu activation function\n",
    "- RMSprop optimizer (learning rate = 0.001)\n",
    "- Vocabulary size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model constants\n",
    "vocab_size = 10000\n",
    "embedding_dim = 356\n",
    "\n",
    "# Build the GRU model\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
    "embedding = layers.Embedding(input_dim=vocab_size,\n",
    "                             output_dim=embedding_dim,\n",
    "                             mask_zero=True, name=\"embedding\")(inputs)\n",
    "x = layers.GRU(64, return_sequences=True, name=\"gru_1\")(embedding)\n",
    "x = layers.GRU(32, name=\"gru_2\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Two_Layer_GRU_TF-TV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "**8. Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Two_Layer_GRU_TF-TV\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Two_Layer_GRU_TF-TV\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">356</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,560,000</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">81,024</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m356\u001b[0m) │  \u001b[38;5;34m3,560,000\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m81,024\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m9,408\u001b[0m │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,650,465</span> (13.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,650,465\u001b[0m (13.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,650,465</span> (13.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,650,465\u001b[0m (13.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**9. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3s/step - accuracy: 0.4656 - loss: 0.6931 - val_accuracy: 0.4500 - val_loss: 0.6957\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.7844 - loss: 0.6711 - val_accuracy: 0.4000 - val_loss: 0.7000\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3s/step - accuracy: 0.8586 - loss: 0.6390 - val_accuracy: 0.5000 - val_loss: 0.7109\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.8852 - loss: 0.5775 - val_accuracy: 0.5000 - val_loss: 0.7385\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - accuracy: 0.9055 - loss: 0.4690 - val_accuracy: 0.4500 - val_loss: 0.7910\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3s/step - accuracy: 0.9258 - loss: 0.3145 - val_accuracy: 0.4500 - val_loss: 0.8525\n",
      "Training Time: 46.4211802482605 seconds\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing models in memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define the callbacks for the model training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"EXP_3_GRU_TF_TV.keras\", save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = model.fit(int_train_ds,\n",
    "                    validation_data=int_val_ds,\n",
    "                    epochs=20,\n",
    "                    callbacks=callbacks)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the training time\n",
    "print(f\"Training Time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**10. Evaluate the model on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602ms/step - accuracy: 0.7000 - loss: 0.6735\n",
      "Test Loss: 0.6734539270401001, Test Accuracy: 0.699999988079071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1.2.2 Extract the training history and add all evaluation metrics into a history DataFrame\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXP</th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>GRU w/ TextVectorization Embeddings</td>\n",
       "      <td>0.29904</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.852489</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.673454</td>\n",
       "      <td>0.7</td>\n",
       "      <td>46.42118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXP                                Model  Training Loss  Training Accuracy  \\\n",
       "0    3  GRU w/ TextVectorization Embeddings        0.29904             0.9375   \n",
       "\n",
       "   Validation Loss  Validation Accuracy  Test Loss  Test Accuracy  \\\n",
       "0         0.852489                 0.45   0.673454            0.7   \n",
       "\n",
       "   Training Time  \n",
       "0       46.42118  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model\n",
    "model = tf.keras.models.load_model(\"EXP_3_GRU_TF_TV.keras\")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(int_test_ds)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Get the training loss, validation loss, training accuracy, and validation accuracy from the history object\n",
    "training_loss = history.history['loss'][-1] # the -1 index gets the last epoch\n",
    "validation_loss = history.history['val_loss'][-1]\n",
    "training_accuracy = history.history['accuracy'][-1]\n",
    "validation_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "\"\"\"\n",
    "1.2.2 Extract the training history and add all evaluation metrics into a history DataFrame\n",
    "\"\"\"\n",
    "# Extract the training history into a pandas DataFrame\n",
    "history_df = pd.DataFrame({\n",
    "    'EXP': [3],\n",
    "    'Model': ['GRU w/ TextVectorization Embeddings'],\n",
    "    'Training Loss': [training_loss],\n",
    "    'Training Accuracy': [training_accuracy],\n",
    "    'Validation Loss': [validation_loss],\n",
    "    'Validation Accuracy': [validation_accuracy],\n",
    "    'Test Loss': [test_loss],\n",
    "    'Test Accuracy': [test_accuracy],\n",
    "    'Training Time': [training_time]\n",
    "})\n",
    "\n",
    "# Inspect the history DataFrame\n",
    "history_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
